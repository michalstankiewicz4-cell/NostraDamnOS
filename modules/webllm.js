// Integracja z WebLLM (@mlc-ai/web-llm) dla lokalnego modelu AI w przeglƒÖdarce
// Wymaga WebGPU (Chrome 113+, Edge 113+)

let engine = null;
let isLoading = false;
let currentModel = null;

/**
 * Dostƒôpne modele WebLLM (zoptymalizowane pod WebGPU)
 */
export const WEBLLM_MODELS = [
    { id: 'SmolLM2-360M-Instruct-q4f16_1-MLC', name: 'SmolLM2 360M', size: '~200MB', category: 'nano' },
    { id: 'SmolLM2-1.7B-Instruct-q4f16_1-MLC', name: 'SmolLM2 1.7B', size: '~1GB', category: 'small' },
    { id: 'Llama-3.2-1B-Instruct-q4f16_1-MLC', name: 'Llama 3.2 1B', size: '~700MB', category: 'small' },
    { id: 'Llama-3.2-3B-Instruct-q4f16_1-MLC', name: 'Llama 3.2 3B', size: '~1.8GB', category: 'medium' },
    { id: 'Phi-3.5-mini-instruct-q4f16_1-MLC', name: 'Phi 3.5 Mini 3.8B', size: '~2.2GB', category: 'medium' },
    { id: 'gemma-2-2b-it-q4f16_1-MLC', name: 'Gemma 2 2B', size: '~1.3GB', category: 'small' },
    { id: 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC', name: 'Qwen 2.5 1.5B', size: '~1GB', category: 'small' }
];

/**
 * Sprawd≈∫ czy WebGPU jest dostƒôpne
 */
export function isWebGPUAvailable() {
    return !!navigator.gpu;
}

/**
 * Inicjalizacja WebLLM ‚Äî ≈Çadowanie modelu
 * @param {string} modelId ‚Äî ID modelu z WEBLLM_MODELS
 * @param {Function} onProgress ‚Äî callback(info: {progress: number, text: string})
 */
export async function initWebLLM(modelId = 'SmolLM2-1.7B-Instruct-q4f16_1-MLC', onProgress = null) {
    console.log('üîß Inicjalizacja WebLLM...');

    if (!isWebGPUAvailable()) {
        throw new Error('WebGPU niedostƒôpne. U≈ºyj Chrome 113+ lub Edge 113+.');
    }

    if (isLoading) {
        throw new Error('Model jest ju≈º ≈Çadowany. Poczekaj na zako≈Ñczenie.');
    }

    isLoading = true;

    try {
        // Dynamiczny import @mlc-ai/web-llm
        const { CreateMLCEngine } = await import('https://esm.run/@mlc-ai/web-llm');

        engine = await CreateMLCEngine(modelId, {
            initProgressCallback: (info) => {
                console.log(`[WebLLM] ${info.text || ''} (${Math.round((info.progress || 0) * 100)}%)`);
                if (onProgress) onProgress(info);
            }
        });

        currentModel = modelId;
        isLoading = false;
        console.log(`‚úÖ WebLLM gotowy ‚Äî model: ${modelId}`);
        return true;
    } catch (error) {
        isLoading = false;
        console.error('‚ùå B≈ÇƒÖd inicjalizacji WebLLM:', error);
        throw error;
    }
}

/**
 * Czy silnik jest gotowy?
 */
export function isReady() {
    return !!engine;
}

/**
 * Pobierz aktualny model
 */
export function getCurrentModel() {
    return currentModel;
}

/**
 * Chat completion ‚Äî wy≈õlij prompt i uzyskaj odpowied≈∫
 * @param {string} userMessage ‚Äî wiadomo≈õƒá u≈ºytkownika
 * @param {string} systemPrompt ‚Äî opcjonalny system prompt
 * @param {Object} options ‚Äî {temperature, maxTokens}
 * @returns {string} ‚Äî odpowied≈∫ modelu
 */
export async function chatCompletion(userMessage, systemPrompt = '', options = {}) {
    if (!engine) throw new Error('Model nie za≈Çadowany. Wywo≈Çaj initWebLLM() najpierw.');

    const { temperature = 0.5, maxTokens = 800 } = options;

    const messages = [];
    if (systemPrompt) messages.push({ role: 'system', content: systemPrompt });
    messages.push({ role: 'user', content: userMessage });

    const reply = await engine.chat.completions.create({
        messages,
        temperature,
        max_tokens: maxTokens
    });

    return reply.choices[0]?.message?.content || '';
}

/**
 * Generowanie streszczenia wypowiedzi
 * @param {Array} speeches ‚Äî [{speaker, text, date, party}]
 */
export async function generateSummary(speeches) {
    console.log('üìù Generowanie streszcze≈Ñ...');

    if (!engine) throw new Error('Model nie za≈Çadowany');

    const excerpts = speeches.slice(0, 20).map(s =>
        `[${s.speaker || 'M√≥wca'}] ${(s.text || '').substring(0, 200)}`
    ).join('\n');

    const result = await chatCompletion(
        `Przeanalizuj poni≈ºsze wypowiedzi parlamentarne i napisz zwiƒôz≈Çe podsumowanie (max 200 s≈Ç√≥w) po polsku:\n\n${excerpts}`,
        'Jeste≈õ ekspertem od podsumowywania debat parlamentarnych. Pisz zwiƒô≈∫le i merytorycznie po polsku.'
    );

    return [{ speaker: 'AI', date: new Date().toISOString(), summary: result }];
}

/**
 * Por√≥wnanie wypowiedzi r√≥≈ºnych m√≥wc√≥w
 * @param {Array} speeches ‚Äî [{speaker, text, party}]
 */
export async function compareSpeeches(speeches) {
    console.log('üîÑ Por√≥wnywanie wypowiedzi...');

    if (!engine) throw new Error('Model nie za≈Çadowany');

    const excerpts = speeches.slice(0, 10).map(s =>
        `[${s.speaker || 'M√≥wca'}, ${s.party || '?'}] ${(s.text || '').substring(0, 150)}`
    ).join('\n');

    const result = await chatCompletion(
        `Por√≥wnaj poni≈ºsze wypowiedzi parlamentarne. Wska≈º podobie≈Ñstwa, r√≥≈ºnice i g≈Ç√≥wne tematy. Odpowiedz po polsku w formacie JSON: {similarities: [...], differences: [...], topics: [...]}\n\n${excerpts}`,
        'Jeste≈õ analitykiem parlamentarnym. Odpowiadaj w formacie JSON.'
    );

    try {
        const jsonMatch = result.match(/\{[\s\S]*\}/);
        return jsonMatch ? JSON.parse(jsonMatch[0]) : { similarities: [], differences: [], topics: [], rawAnalysis: result };
    } catch {
        return { similarities: [], differences: [], topics: [], rawAnalysis: result };
    }
}

/**
 * Analiza argumentacji w wypowiedzi
 * @param {Object} speech ‚Äî {text, speaker}
 */
export async function analyzeArgumentation(speech) {
    console.log('üéØ Analiza argumentacji...');

    if (!engine) throw new Error('Model nie za≈Çadowany');

    const result = await chatCompletion(
        `Przeanalizuj argumentacjƒô w poni≈ºszej wypowiedzi parlamentarnej. Zidentyfikuj twierdzenia, dowody i ewentualne b≈Çƒôdy logiczne. Odpowiedz JSON: {claims: [...], evidence: [...], fallacies: [...]}\n\nWypowied≈∫: ${(speech.text || '').substring(0, 500)}`,
        'Jeste≈õ ekspertem od analizy argumentacji i retoryki parlamentarnej. Odpowiadaj po polsku w JSON.'
    );

    try {
        const jsonMatch = result.match(/\{[\s\S]*\}/);
        return jsonMatch ? JSON.parse(jsonMatch[0]) : { claims: [], evidence: [], fallacies: [], rawAnalysis: result };
    } catch {
        return { claims: [], evidence: [], fallacies: [], rawAnalysis: result };
    }
}

/**
 * Analiza retoryki
 * @param {Object} speech ‚Äî {text, speaker}
 */
export async function analyzeRhetoric(speech) {
    console.log('üé≠ Analiza retoryki...');

    if (!engine) throw new Error('Model nie za≈Çadowany');

    const result = await chatCompletion(
        `Przeanalizuj retorykƒô poni≈ºszej wypowiedzi. Okre≈õl ton, perswazyjno≈õƒá (0-1), u≈ºyte ≈õrodki retoryczne. JSON: {devices: [...], tone: "...", persuasiveness: 0.X}\n\n${(speech.text || '').substring(0, 500)}`,
        'Jeste≈õ ekspertem od retoryki politycznej. Odpowiadaj po polsku w JSON.'
    );

    try {
        const jsonMatch = result.match(/\{[\s\S]*\}/);
        return jsonMatch ? JSON.parse(jsonMatch[0]) : { devices: [], tone: 'neutral', persuasiveness: 0.5, rawAnalysis: result };
    } catch {
        return { devices: [], tone: 'neutral', persuasiveness: 0.5, rawAnalysis: result };
    }
}

/**
 * Kontekstowa analiza wypowiedzi
 * @param {Object} speech ‚Äî {text, speaker}
 * @param {Object} context ‚Äî dodatkowy kontekst
 */
export async function analyzeContext(speech, context = {}) {
    console.log('üîç Analiza kontekstu...');

    if (!engine) throw new Error('Model nie za≈Çadowany');

    const ctxStr = context.topic ? `Temat: ${context.topic}. ` : '';
    const result = await chatCompletion(
        `${ctxStr}Oce≈Ñ poni≈ºszƒÖ wypowied≈∫ parlamentarnƒÖ pod kƒÖtem trafno≈õci, sp√≥jno≈õci i wniosk√≥w. JSON: {relevance: 0.X, coherence: 0.X, insights: [...]}\n\n${(speech.text || '').substring(0, 500)}`,
        'Jeste≈õ analitykiem parlamentarnym. Odpowiadaj po polsku w JSON.'
    );

    try {
        const jsonMatch = result.match(/\{[\s\S]*\}/);
        return jsonMatch ? JSON.parse(jsonMatch[0]) : { relevance: 0.5, coherence: 0.5, insights: [], rawAnalysis: result };
    } catch {
        return { relevance: 0.5, coherence: 0.5, insights: [], rawAnalysis: result };
    }
}

/**
 * Zwolnij zasoby silnika
 */
export async function unload() {
    if (engine) {
        try {
            await engine.unload?.();
        } catch { /* ignore */ }
        engine = null;
        currentModel = null;
        console.log('[WebLLM] Silnik zwolniony');
    }
}
